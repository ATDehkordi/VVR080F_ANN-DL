{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b162f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 2.9.0+cu126\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atdehkordi/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('version', torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51244156",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading ForestCanopyHeight from AlphaEarth\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('/home/atdehkordi/PHDLund/PythonProjects_github/VVR080F_ANN-DL/Data_Regression/ForestCanopyHeight/AlphaEarth/30m.xlsx')\n",
    "\n",
    "x_inputs = df.iloc[:, 0:64].to_numpy(dtype = np.float32) \n",
    "y_output = df.iloc[:, -1].to_numpy(dtype = np.float32)\n",
    "\n",
    "print('x shape:', x_inputs.shape)\n",
    "print('y shape:', y_output.shape)\n",
    "\n",
    "print('x min:', np.min(x_inputs))\n",
    "print('x max:', np.max(x_inputs))\n",
    "\n",
    "print('y min:', np.min(y_output))\n",
    "print('y max:', np.max(y_output))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(y_output, bins=10)\n",
    "plt.xlabel('FCH [m]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If some ranges of the y target are having very low number of samples, here can be removed:\n",
    "\n",
    "index = y_output<=40\n",
    "y_output = y_output[index]\n",
    "x_inputs = x_inputs[index, :]\n",
    "\n",
    "print('y shape:', y_output.shape)\n",
    "print('y min:', np.min(y_output))\n",
    "print('y max:', np.max(y_output))\n",
    "\n",
    "plt.hist(y_output, bins=10)\n",
    "plt.xlabel('FCH [m]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def log_scale_transform(X, Y):\n",
    "    \n",
    "    shift_value_X = np.abs(np.min(X)) + 1  # Shift X to ensure positive values\n",
    "    shift_value_Y = np.abs(np.min(Y)) + 1  # Shift Y similarly\n",
    "    \n",
    "    X_shifted = X + shift_value_X\n",
    "    Y_shifted = Y + shift_value_Y\n",
    "    \n",
    "    X_log = np.log(X_shifted)\n",
    "    Y_log = np.log(Y_shifted)\n",
    "\n",
    "    s1 = np.arange(X_log.shape[0])\n",
    "    np.random.shuffle(s1)\n",
    "    X_log = X_log[s1, :]\n",
    "    Y_log = Y_log[s1]\n",
    "    \n",
    "    transformerX = RobustScaler().fit(X_log)\n",
    "    X_trans = transformerX.transform(X_log)\n",
    "    min_max_scalerX = MinMaxScaler().fit(X_trans)\n",
    "    X_trans2 = min_max_scalerX.transform(X_trans)\n",
    "\n",
    "    transformerY = RobustScaler().fit(np.reshape(Y_log,(-1,1)))\n",
    "    Y_trans = transformerY.transform(np.reshape(Y_log,(-1,1)))\n",
    "    min_max_scalerY = MinMaxScaler().fit(Y_trans)\n",
    "    Y_trans2 = min_max_scalerY.transform(Y_trans)\n",
    "\n",
    "    return X_trans2, Y_trans2, transformerX, transformerY, min_max_scalerX, min_max_scalerY, shift_value_X, shift_value_Y, s1\n",
    "\n",
    "def ytest_to_initial_scale(ytest, min_max_scalerY_turb, transformerY_turb, shift_value_Y):\n",
    "    ytest_inv1 = min_max_scalerY_turb.inverse_transform(np.reshape(ytest, (-1, 1)))\n",
    "    ytest_inv2 = transformerY_turb.inverse_transform(ytest_inv1)\n",
    "    ytest_inv2 = np.exp(ytest_inv2)\n",
    "    ytest_rescaled = ytest_inv2 - shift_value_Y\n",
    "    return ytest_rescaled\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def r_squared(y, y_hat):\n",
    " ''' Logarithmic R^2 '''\n",
    " slope_, intercept_, r_value, p_value, std_err = stats.linregress(y, y_hat)\n",
    " return r_value**2 * 100\n",
    "\n",
    "def mape(y, y_hat):\n",
    " ''' Mean Absolute Percentage Error '''\n",
    " return 100 * np.mean(np.abs((y - y_hat) / y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rescaled, y_rescaled, transformerX, transformerY, min_max_scalerX, min_max_scalerY, shift_value_X, shift_value_Y, s1 = log_scale_transform(x_inputs, y_output)\n",
    "y_rescaled = np.squeeze(y_rescaled)\n",
    "\n",
    "print('data_total,' , y_rescaled.shape)\n",
    "print('min,' ,np.min(y_rescaled))\n",
    "print('max,' ,np.max(y_rescaled))\n",
    "print('mean,' ,np.mean(y_rescaled))\n",
    "print('std,' ,np.std(y_rescaled))\n",
    "\n",
    "plt.hist(y_rescaled, bins=10)\n",
    "plt.xlabel('Transformed FCH [m]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To see the performance of a simple RF\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, GridSearchCV, LeaveOneOut, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --------------------------\n",
    "# 0) Putting n% of the data aside only for testing!\n",
    "# --------------------------\n",
    "\n",
    "test_size = 0.3\n",
    "\n",
    "print(\"###################### Step 1\")\n",
    "print(f\"Train and Test split: {test_size * 100}% of dataset is used only for testing...\")\n",
    "\n",
    "## If it is not your first run, activate the below parts for loading: \n",
    "\n",
    "file_path = '/home/atdehkordi/PHDLund/PythonProjects_github/VVR080F_ANN-DL/Data_Regression/ForestCanopyHeight/AlphaEarth/30m_train_test/'\n",
    "\n",
    "X_train = np.load(file_path + 'X_train.npy')\n",
    "X_test = np.load(file_path + 'X_test.npy')\n",
    "y_train = np.squeeze(np.load(file_path + 'y_train.npy').reshape(-1,1))\n",
    "y_test = np.squeeze(np.load(file_path + 'y_test.npy').reshape(-1,1))\n",
    "\n",
    "\n",
    "## If it is your first run, comment the below parts for saving: \n",
    "\n",
    "# indices = np.arange(len(x_rescaled))\n",
    "\n",
    "# X_train, X_test, y_train, y_test, train_idx, test_idx  = train_test_split(\n",
    "#     x_rescaled, \n",
    "#     y_rescaled, \n",
    "#     indices,\n",
    "#     test_size=test_size, \n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# file_path = '/home/atdehkordi/PHDLund/PythonProjects_github/VVR080F_ANN-DL/Data_Regression/ForestCanopyHeight/AlphaEarth/30m_train_test/'\n",
    "\n",
    "# np.save(file_path + 'X_train.npy', X_train)\n",
    "# np.save(file_path + 'X_test.npy', X_test)\n",
    "# np.save(file_path + 'y_train.npy', y_train)\n",
    "# np.save(file_path + 'y_test.npy', y_test)\n",
    "\n",
    "# --------------------------\n",
    "# 1) Tune hyperparameters using, say, 5-fold CV on training data\n",
    "# --------------------------\n",
    "\n",
    "print(\"###################### Step 2\")\n",
    "print(f\"Hyperparameter tuning using k_fold validation on training data (100% - test_size = {100-test_size*100}%)...\")\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "rf_base = RandomForestRegressor(random_state=42)\n",
    "\n",
    "kfold = KFold(n_splits= n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=kfold,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters using k-fold validation on training data:\", grid_search.best_params_)\n",
    "\n",
    "print(\"###################### Step 3\")\n",
    "print(\"Model validation using the best parameters found and k-fold method on training data...\")\n",
    "\n",
    "best_rf = RandomForestRegressor(n_estimators = grid_search.best_params_['n_estimators'],  \n",
    "                                max_depth = grid_search.best_params_['max_depth'], \n",
    "                                min_samples_leaf = grid_search.best_params_['min_samples_leaf'], \n",
    "                                min_samples_split = grid_search.best_params_['min_samples_split'], \n",
    "                                random_state=42)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "mape_list_training = []\n",
    "r2_list_training   = []\n",
    "\n",
    "mape_list_test = []\n",
    "r2_list_test  = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_train):\n",
    "\n",
    "    print(f'######## Fold{counter}')\n",
    "    counter += 1\n",
    "    \n",
    "    x_train_kfold, x_test_kfold = X_train[train_idx], X_train[test_idx]\n",
    "    y_train_kfold, y_test_kfold = y_train[train_idx], y_train[test_idx]\n",
    "    \n",
    "    # Fit the best RF on the training fold\n",
    "    best_rf.fit(x_train_kfold, y_train_kfold)\n",
    "    \n",
    "    # Predict on the test fold\n",
    "    y_pred_rescaled_kfold = best_rf.predict(x_test_kfold)\n",
    "    \n",
    "    # Transform y_test and predictions back to original scale\n",
    "    y_pred_original_kfold = np.squeeze(ytest_to_initial_scale(\n",
    "        y_pred_rescaled_kfold, \n",
    "        min_max_scalerY, \n",
    "        transformerY, \n",
    "        shift_value_Y\n",
    "    ))\n",
    "    y_test_original_kfold = np.squeeze(ytest_to_initial_scale(\n",
    "        y_test_kfold, \n",
    "        min_max_scalerY, \n",
    "        transformerY, \n",
    "        shift_value_Y\n",
    "    ))\n",
    "\n",
    "    # Compute metrics on THIS fold\n",
    "    mape_list_training.append(mape(y_test_original_kfold, y_pred_original_kfold))\n",
    "    r2_list_training.append(r_squared(y_test_original_kfold, y_pred_original_kfold))\n",
    "\n",
    "    # Predict on the test fold\n",
    "    y_pred_rescaled = best_rf.predict(X_test)\n",
    "    \n",
    "    # Transform y_test and predictions back to original scale\n",
    "    y_pred_original = np.squeeze(ytest_to_initial_scale(\n",
    "        y_pred_rescaled, \n",
    "        min_max_scalerY, \n",
    "        transformerY, \n",
    "        shift_value_Y\n",
    "    ))\n",
    "    y_test_original = np.squeeze(ytest_to_initial_scale(\n",
    "        y_test, \n",
    "        min_max_scalerY, \n",
    "        transformerY, \n",
    "        shift_value_Y\n",
    "    ))\n",
    "    # Compute metrics on THIS fold\n",
    "    mape_list_test.append(mape(y_test_original, y_pred_original))\n",
    "    r2_list_test.append(r_squared(y_test_original, y_pred_original))\n",
    "\n",
    "# Report average metrics across folds\n",
    "print(\"Training : Mean MAPE (K-fold):\", np.mean(mape_list_training))\n",
    "print(\"Training : Std  MAPE (K-fold):\", np.std(mape_list_training))\n",
    "print(\"Training : Mean R²   (K-fold):\", np.mean(r2_list_training))\n",
    "print(\"Training : Std  R²   (K-fold):\", np.std(r2_list_training))\n",
    "\n",
    "# Report average metrics across folds\n",
    "print(\"Test : Mean MAPE (K-fold):\", np.mean(mape_list_test))\n",
    "print(\"Test : Std  MAPE (K-fold):\", np.std(mape_list_test))\n",
    "print(\"Test : Mean R²   (K-fold):\", np.mean(r2_list_test))\n",
    "print(\"Test : Std  R²   (K-fold):\", np.std(r2_list_test))\n",
    "\n",
    "print(\"###################### Step 3\")\n",
    "print(\"Model validation using the best parameters found and whole training data on test data only\")\n",
    "\n",
    "best_rf = RandomForestRegressor(n_estimators = grid_search.best_params_['n_estimators'],  \n",
    "                                max_depth = grid_search.best_params_['max_depth'], \n",
    "                                min_samples_leaf = grid_search.best_params_['min_samples_leaf'], \n",
    "                                min_samples_split = grid_search.best_params_['min_samples_split'], \n",
    "                                random_state=42)\n",
    "\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rescaled = best_rf.predict(X_test)\n",
    "\n",
    "y_pred_original = np.squeeze(ytest_to_initial_scale(\n",
    "    y_pred_rescaled, min_max_scalerY, transformerY, shift_value_Y\n",
    "))\n",
    "y_test_original = np.squeeze(ytest_to_initial_scale(\n",
    "    y_test, min_max_scalerY, transformerY, shift_value_Y\n",
    "))\n",
    "\n",
    "MAPE_value = mape(y_test_original, y_pred_original)\n",
    "r2_value = r_squared(y_test_original, y_pred_original)\n",
    "\n",
    "print(\"MAPE:\", MAPE_value)\n",
    "print(\"R2  :\", r2_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be53045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Chl-a plots\n",
    "\n",
    "import matplotlib\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "filename = \"Best_RF.jpg\"\n",
    "\n",
    "matplotlib.rcParams['axes.linewidth'] = 1.5\n",
    "\n",
    "y = np.squeeze(y_pred_original)\n",
    "x = np.squeeze(y_test_original)\n",
    "\n",
    "mape_test = mape(np.squeeze(x), np.squeeze(y))\n",
    "r2_test = r_squared(np.squeeze(x), np.squeeze(y))\n",
    "\n",
    "print('')\n",
    "print('MAPE = ', mape_test)\n",
    "print('R2 = ', r2_test)\n",
    "print('')\n",
    "\n",
    "# Calculate the point density\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "# Sort the points by density, so that the densest points are plotted last\n",
    "idx = z.argsort()\n",
    "x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "\n",
    "ax.plot([0,50], [0,50], color='grey', linestyle='dashed', linewidth=3) #for DO\n",
    "\n",
    "ax.scatter(x, y, c=z, s=300, cmap='jet')\n",
    "\n",
    "ax.set_xlim([0, 40])\n",
    "ax.set_ylim([0, 40])\n",
    "\n",
    "ax.set_xlabel(\"In-situ values\", fontsize=20, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"RF estimates\", fontsize=20, fontweight=\"bold\")\n",
    "\n",
    "plt.xticks(fontsize=25, fontweight='bold');\n",
    "plt.yticks(fontsize=25, fontweight='bold');\n",
    "\n",
    "# fig.savefig(filename, dpi=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0d59e",
   "metadata": {},
   "source": [
    "### In terms of training a self-attentin module, there is a need to define two terms:\n",
    "1- tokens\n",
    "2- embeddings\n",
    "\n",
    "1-tokens: token is a unit that your model reads to understand a sequence. for example in NLP, words are sometimes tokens. In images, pathches of images can be considered as tokens.\n",
    "\n",
    "2- an embedding is a vector that represents a token. Tokens are like IDs so if a token has an id of 25 and the other one is 26 there is no meaningful relationship between 25 and 26 like 26>25 but the embeddings carry meaningful informations about the tokens.\n",
    "\n",
    "Tokenization is really important because if you consider a number for each word (token) then for the same word with different meaning, how you want to descriminate it. In other owrds, for the word leave in leave a message or leave the place, you have a similar token but they have different meanings. That is why you have to have both token and embedding. Token for cutting into smaller parts, embedding for meaning.\n",
    "Embedding can also be determined as the word/token representation\n",
    "Then for assessing the similarity, cosine of these embeddings is fine.\n",
    "\n",
    "Self-attention understands relations between tokens using their embeddings.\n",
    "\n",
    "\n",
    "When dealing with 1d data like a timeseries of a specific parameter or a spectral band of S2 data for WQ estimation, there is 2 ways to consider tokens and embeddings:\n",
    "\n",
    "a) you can consider each feature as a token so for example you will have 10 tokens for 10 spectral bands of S2 data, each of it has only 1 embedding (band value)\n",
    "\n",
    "    In this case you can also use a positional embedding so the attention also understands about the location of each feature.\n",
    "    It is also recommended to increase the embedding size (not put it to 1) because it gives attention richer features to mix (add a small linear projection Linear(1 → d_model) to turn each scalar into a richer vector (e.g., d_model=32).)\n",
    "\n",
    "\n",
    "b) you can consider you have 1 token and 64 features. in this case, transformers are useless. Because they are designed to extract the relations between tokens not embeddings.\n",
    "\n",
    "\n",
    "### So, as a general rule: \n",
    "#### Self-attention only helps when there’s more than one token so  when your data has a sequence or structure across positions.\n",
    "\n",
    "c) When I have for example 64 features, it is also possible to change it to a 8x8 2D token, where each feature becomes a token.\n",
    "\n",
    "\n",
    "d) when you have a feature vector of 64 you can also make it 4x4 wit each embedding of 4 or, 2x2 with each embedding of 16 (in this case you can select different combination-infinite cases- so maybe it is a good idea to incorporate kmeans clustering but how: a very good idea for research)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009bcc56",
   "metadata": {},
   "source": [
    "### Now let's see how self attention works:\n",
    "\n",
    "suppose that we have n_token=seq_len and n_emb=d_model\n",
    "so: x.shape = (batch_size, seq_len, d_model)\n",
    "\n",
    "Self-attention wants to say how strongly one token must look at the other tokens.\n",
    "To do this, each token must send an information to the other tokens.\n",
    "This is done by computing three projections:\n",
    "\n",
    "Q=XWQ​,K=XWK​,V=XWV​ (Q = queries, K = keys, V = values)\n",
    "In the above equation, Wq, Wk, Wv are learnable weights with size: d_model, d_k (we usually set: d_k = d_model/num_of_heads) where num_of_heads indicates how many self-attention modules we want to use.\n",
    "\n",
    "as an example:\n",
    "\n",
    "| Symbol              | Shape    | Meaning                                  |\n",
    "| ------------------- | -------- | ---------------------------------------- |\n",
    "| `X`                 | (64, 32) | 64 tokens, 32-dim embedding each         |\n",
    "| `W_Q`, `W_K`, `W_V` | (32, 32) | projection weights                       |\n",
    "| `Q`, `K`, `V`       | (64, 32) | 64 query/key/value vectors (1 per token) |\n",
    "\n",
    "then, we need to compute similarities betwee Q and K:\n",
    "\n",
    "Scores=QK.T (this is done using dot product which easily gives the similarities with somehow a cosine formula)\n",
    "in the above example, Scores will be 64*64 (it gives the similarity of tokens all to each other or how much token i “attends to” token j before normalization.)\n",
    "\n",
    "\n",
    "Then we apply softmax row-wise: A=softmax(​QKT​) Now each row A[i, :] sums to 1. \n",
    "A diagonal entry (A[i, i]) = how much a token looks at itself.\n",
    "Off-diagonal (A[i, j]) = how much token i looks at token j.\n",
    "Shape of A = (64, 64).\n",
    "\n",
    "Now we weigh the value vectors (V) using attention weights A:\n",
    "Output=AV Result: (64, 32) this is equivalent to initial X which was 64, 32 which means now for each token we have a new weighted embeddings\n",
    "\n",
    "\n",
    "if you do a multi-head attention, like 8 heads: d_k = 32 / 8 = 4\n",
    "so each head runs its own attention giving (64, 4) and then concatenates 8 heads to give (64, 32) output\n",
    "\n",
    "\n",
    "So, When you have only one token (seq_len = 1) So nothing happens — the token only attends to itself with weight 1.\n",
    "That’s why you need multiple tokens for attention to learn relationships.\n",
    "\n",
    "If your input has 64 features and you treat each feature as a token (seq_len=64), So attention is literally learning which features are most related to which for predicting the target.\n",
    "\n",
    "\n",
    "In the transformrs, there is also a term : positional embedding\n",
    "because of the attention formula: Attention(Q,K,V)=softmax(QKT​)V, it does not inherently know which token came first, second, or last. So if you shuffled your tokens, you’d get the same output distribution — which is a problem for:\n",
    "text (word order matters),\n",
    "time series (temporal order matters),\n",
    "spatial data (feature positions matter).\n",
    "Therefore, Transformers need a way to inject positional information into the model.\n",
    "\n",
    "So, we add (or sometimes concatenate) a position-dependent vector \n",
    "\n",
    "𝑋=𝑋+𝑃 where P is (seq, emb) meaning that the final output (x+P) encodes both:\n",
    "its content (word/feature meaning)\n",
    "its position (where it sits in the sequence)\n",
    "\n",
    "Positional encoding methods:\n",
    "1- Classical sinusoidal positional encoding\n",
    "2- Learned positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5402ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### simple self-attention for 1d data (each feature is considered as a single token so the embedding size will be 1,\n",
    "#### I have also added a case with/without positional embedding (learnable/no-learnable) and also increasing the embedding size (yes/no)\n",
    "#### and also to map the alphaearth fetures to 2D like 4*4 with 4 embeddings and so on.)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len = 64, attn_dropout = 0, hidden = 128, output_dim = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len # in case of using alphaearthnet you will have 64 features every of them with with one feature (embed=1)\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.hidden = hidden\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(embed_dim=1, num_heads=1, dropout=attn_dropout, batch_first=True) # embed_dim means the dimensionality of both the input and output embeddings\n",
    "        # MultiheadAttention genertaes the Q, K,V from inputs (it requires 3 inputs, if you give all 3, the Q, K, and V are computed from that)\n",
    "\n",
    "        # Simple regression head\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),              # (B, 64, 1) -> (B, 64)\n",
    "            nn.LayerNorm(seq_len),\n",
    "            nn.Linear(seq_len, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, output_dim)\n",
    "        )\n",
    "\n",
    "        # pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)  # x is currenly (n_samples, n_features), I will make it (n_samples, n_features, 1) so n_features is the number of tokens and 1 is the embedding size\n",
    "        \n",
    "        attn_out, _ = self.attn(x, x, x)  # (B, 64, 1), self.attn gets three inputs of Q, V, and K where in our case they are all from data\n",
    "        out = self.mlp(attn_out)          # (B, 1)\n",
    "        return out\n",
    "        \n",
    "        # pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44373d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "import csv\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "hyperparameter_csv_file = 'SelfAttention_hyperparams.csv'\n",
    "\n",
    "def write_csv(results_dict, csv_file):\n",
    "    headers = list(results_dict.keys())\n",
    "    rows = zip(*results_dict.values())\n",
    "\n",
    "    with open(csv_file, mode='w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Write the header row (dictionary keys)\n",
    "        writer.writerow(headers)\n",
    "        # Write each row of values\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "param_grid = {\n",
    "    'epochs': [100],\n",
    "    'batch_size': [4, 8, 16, 32, 64, 128],\n",
    "    'lr': [0.0005, 0.0001, 0.005, 0.001],\n",
    "    'positional_encode':[0, 1, 2], # 0: None, 1: Sinsuidal, 2: Learnable,\n",
    "    'increase_embedd_size': [0, 8], #0: None, any other number like 4, 8: change embed size to a higher dimension\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'epochs': [100, 200, 300],\n",
    "    'batch_size': [4, 8, 16, 32, 64, 128],\n",
    "    'lr': [0.0005, 0.0001, 0.005, 0.001],\n",
    "}\n",
    "\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Number of all possible grid search scenarios: {len(list(ParameterGrid(param_grid)))}\")\n",
    "\n",
    "results = {key:[] for key in param_grid.keys()}\n",
    "results['Train_MAPE'] = []\n",
    "results['Train_R2'] = []\n",
    "results['Test_MAPE'] = []\n",
    "results['Test_R2'] = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for params in param_combinations:\n",
    "\n",
    "    print(f\"\\n####### Grid {counter}\")\n",
    "    counter = counter + 1\n",
    "\n",
    "    params_dict = dict(zip(list(param_grid.keys()), params))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = params_dict['batch_size'], shuffle = True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = params_dict['batch_size'], shuffle = True)\n",
    "\n",
    "    model = SelfAttention()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = params_dict['lr'])\n",
    "\n",
    "    print(\"## Training\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(params_dict['epochs']):\n",
    "        for batch_x_train, batch_y_train in train_loader:\n",
    "            batch_x_train, batch_y_train = batch_x_train.to(device), batch_y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            batch_y_predictions = model(batch_x_train)\n",
    "\n",
    "            loss = criterion(batch_y_predictions, batch_y_train)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(\"## Evaluation\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        predictions_test, truth_test, predictions_train, truth_train = [], [], [], []\n",
    "\n",
    "        for batch_x_test, batch_y_test in test_loader:\n",
    "\n",
    "            batch_x_test, batch_y_test = batch_x_test.to(device), batch_y_test.to(device)\n",
    "            preds_test = model(batch_x_test)\n",
    "            predictions_test.append(preds_test.cpu())\n",
    "            truth_test.append(batch_y_test.cpu())\n",
    "\n",
    "        predictions_test = torch.cat(predictions_test).numpy()\n",
    "        predictions_test = ytest_to_initial_scale(predictions_test, min_max_scalerY, transformerY, shift_value_Y)\n",
    "        truth_test = torch.cat(truth_test).numpy()\n",
    "        truth_test = ytest_to_initial_scale(truth_test, min_max_scalerY, transformerY, shift_value_Y)\n",
    "\n",
    "        mape_test = mape(np.squeeze(truth_test), np.squeeze(predictions_test))\n",
    "        r2_test = r_squared(np.squeeze(truth_test), np.squeeze(predictions_test))\n",
    "\n",
    "        for batch_x_train, batch_y_train in train_loader:\n",
    "\n",
    "            batch_x_train, batch_y_train = batch_x_train.to(device), batch_y_train.to(device)\n",
    "            preds_train = model(batch_x_train)\n",
    "            predictions_train.append(preds_train.cpu())\n",
    "            truth_train.append(batch_y_train.cpu())\n",
    "\n",
    "        predictions_train = torch.cat(predictions_train).numpy()\n",
    "        predictions_train = ytest_to_initial_scale(predictions_train, min_max_scalerY, transformerY, shift_value_Y)\n",
    "        truth_train = torch.cat(truth_train).numpy()\n",
    "        truth_train = ytest_to_initial_scale(truth_train, min_max_scalerY, transformerY, shift_value_Y)\n",
    "\n",
    "        mape_train = mape(np.squeeze(truth_train), np.squeeze(predictions_train))\n",
    "        r2_train = r_squared(np.squeeze(truth_train), np.squeeze(predictions_train))\n",
    "\n",
    "        # Automate storing the parameter values and performance metrics\n",
    "    for key, value in zip(list(param_grid.keys()), params):\n",
    "        results[key].append(value)\n",
    "    results[\"Test_MAPE\"].append(mape_test)\n",
    "    results[\"Test_R2\"].append(r2_test)\n",
    "    results[\"Train_MAPE\"].append(mape_train)\n",
    "    results[\"Train_R2\"].append(r2_train)\n",
    "\n",
    "    write_csv(results, hyperparameter_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a71010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
